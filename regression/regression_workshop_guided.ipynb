{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Retail E-Commerce\n",
    "This workshop focuses on predicting how much money customers spend online shopping. For this notebook, we're going to use an e-commerce dataset of online shopping found on [Kaggle](https://www.kaggle.com/kolawale/focusing-on-mobile-app-or-website).\n",
    "\n",
    "Your task is to predict the yearly amount a customer will spend on your site given a number of factors such as time on website, time on app, etc.\n",
    "\n",
    "Our aim is to practice linear regression analysis by predicting some numerical value for the yearly amount of money spent online based on these different customer shopping behaviors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Useful Jupyter Shortcuts\n",
    "\n",
    "Below is a (far from comprehensive) shortcut list for Jupyter Notebooks:\n",
    "\n",
    "* `shift` + `Enter`: Run cell\n",
    "* `enter`: Enter *write* mode\n",
    "* `esc`: Exit *write* mode and enter *navigation* mode\n",
    "* (in nav. mode) `a`: Add cell above current cell\n",
    "* (in nav. mode) `b`: Add cell below current cell\n",
    "* (in nav. mode) `m`: Turn current cell into text\n",
    "* (in nav. mode) `y`: Turn current cell into code\n",
    "* (in nav. mode) `d` x2: Delete current cell\n",
    "* `tab`: Tab completion\n",
    "* `shift` + `tab`: Show documentation / parameters\n",
    "\n",
    "Some Jupyter / iPython magic:\n",
    "\n",
    "* Add `?` at the beginning of a command to see documentation for the function/method/etc\n",
    "* Add `%matplotlib inline` at the beginning of your notebook for in-line plots\n",
    "\n",
    "And some useful configurations:\n",
    "\n",
    "* `plt.rcParams['figure.figsize'] = (18.0, 12.0)`: Set the default matplotlib figure size\n",
    "* `pd.set_option('display.max_columns', None)`: Asks pandas to show us **all** columns in dataframes\n",
    "\n",
    "To get rid of the margin and make the cells cover 100% of the window width run the below code block:\n",
    "```\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sourcing the Data\n",
    "We'll start our analysis by sourcing the data. But before we get started, we need to import the necessary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data manipulation package\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many columns and rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of columns and rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the first few columns of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How clean is our data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been loaded successfully and we've learned a bit about its characteristics. In this section we will first make use of Simple Linear Regression to try to predict our target. As such our first task identify our target variable and select one of the remaining independent variables as our predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should we use as dependent and independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import plotting library\n",
    "\n",
    "\n",
    "# Create a pairplot to visualise the relationships between our variables & the distributions of our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplots are visually appealing, but they're slow, not ideal for large datasets, and bombard us with a lot of information.\n",
    "\n",
    "That said, we can use them today to point us in the right direction of better understanding the relationships in the dataset.\n",
    "\n",
    "On the main diagonal, we can see a different kind of chart compared to the rest of the plot. Because it doesn't make sense to produce a scatterplot of variable with itself (there's no new information to be found there - it will be perfectly correlated) the plots instead show a distribution of values for each column, so that you can check the shape of every distribution and/or immediately spot if you have low-variability columns (e.g. categorical variable with values present mainly for only 1 category). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "There are some nice options available in `sns.pairplot` that can make our plots prettier and more informative:\n",
    "\n",
    "* Since the plots are symmetrical we can set `corner=True` to only show the plots **below** the diagonal.\n",
    "* We can substitute the diagonal histograms for density plots with `diag_kind='kde'`.\n",
    "* We can have `seaborn` compute simple linear regressions 'on-the-fly' and plot the regression lines with `kind='reg'`.\n",
    "* We can change the marker type to `+` with `markers='+'` (feel free to experiment with different markers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a pairplot with the recommended parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the numerical variables (notice how `pairplot` automatically excludes non-numerical variables!) it seems clear that we want to use `Yearly Amount Spent` as our dependent variable. As for the independent variable it seems like the best candidate is `Length of Membership`, as it shows a clear linear relationship with our dependent variable.\n",
    "\n",
    "Now that we've selected our independent and dependent variables, let's split them into an `X` and a `y` variable respectively, and check the shapes to verify that the independent variable is in **matrix** form and the dependent variable in **vector** form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the independent and dependent variables into X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the shapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the independent variable, is in matrix form, defined by two dimensions: rows and columns, even if we have only one column in this case. The dependent variable is correctly encoded as a vector, defined only by its length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now split our data into a training and a testing set. Load the necessary function from `sklearn` and split the data into 20% test and 80% train. Set the random state to the number 23, for reproducibility. As always check the shapes to verify it all worked out ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into 80/20 train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant function from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train/test split with random_state=23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a clean train/test split we can start building our simple linear regression model. Import the relevant class from `sklearn`, create your model variable and fit it using the training data. Then use the fitted model to generate predictions on **both** the train and test sets and use those predictions to visualise the regression line (with the train set predictions) and calculate error metrics (with the test set predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and fit a Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine and interpret the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View the slope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions for the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train set points, test set points, and regression line\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant sklearn functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Mean Absolute Error`(MAE) and the `Mean Squared Error` (MSE) are similar in interpretation, despite different calculations and how they treat outliers. The MAE calculates a residual for every data point and takes the absolute value of each so that negative and positive residuals do not cancel out. Then, it takes the average of all these residuals. The MSE does the same thing, but squares the difference and then sums them all rather than relying on the absolute value. Both metrics effectively tell us how well our model predicts the data. The scores range from 0 to infinity, with smaller values indicating the model is doing a good job predicting the output, while a value of 0 means that the model is a perfect predictor of the response variable. The `Root Mean Squared Error` (RMSE) is the square root of the MSE. By taking the square root, the units match those of the output variable, which make interpretation a bit easier.\n",
    "\n",
    "The maths behind each of these metrics is explained in-depth [here](https://www.dataquest.io/blog/understanding-regression-error-metrics/).\n",
    "\n",
    "The average absolute error is ~£51 and the spread of the error, as measured by the RMSE is ~£59. Remember that RMSE penalises big errors more than MAE so it is always the case that RMSE > MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1><center>Break</center></h1>\n",
    "<center>Go back to the workshop to learn about more linear regression assumptions!</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Linear Regression Assumptions\n",
    "\n",
    "As we have seen, there are 4 major assumptions that we'll cover in this module, namely:\n",
    "\n",
    "* Linearity\n",
    "* Normally Distribution of Residuals\n",
    "* Homoscedasticity\n",
    "* Independence\n",
    "\n",
    "In this section we will walk through each of these assumptions and see how to check them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity\n",
    "\n",
    "Linearity is the one assumption from the list that we can check **before** even building the model. In this very notebook, in the step where we had to select the independent variable, we were implicitly selecting the variable that best fits this assumption.\n",
    "\n",
    "Linearity states that the relationship between the independent and dependent variable should be linear. Let us bring back the pairplot from the previous section and verify that the variable we have selected is linearly related to our dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring back the pairplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the four independent variable candidates above, it is clear that `Length of Membership` (the one we selected) has a strong linear relationship with the target. Other variables such as `Avg. Session Length` and `Time on App` also have a linear relationship although not as strong. `Time on Website`, however, does not exhibit a linear relationship with the target and so we should be wary of using it as an independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normally Distributed Residuals\n",
    "\n",
    "This optional assumption states that the residuals should be normally distributed. While not very important for prediction, the normal distribution of residuals is crucial when determining confidence intervals for our coefficients, something we will look at closely in the online practice.\n",
    "\n",
    "There are many different tests designed to test whether a distribution is normal. We will focus on visually inspecting a histogram of residuals, but we also introduce, as supplemental material the use of normal Q-Q plots.\n",
    "\n",
    "For assumption validation we'd like to use all available data. We can go back to our original `X` and `y` variables defined right before the train/test split to get access to **all** data without having to bother with concatenation (or combining variables). \n",
    "\n",
    "We'd start by generating predictions for the entire dataset (`X`), then calculate the residuals (defined as **true - predicted**), and then plotting a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions for the original X variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the number of bins in the histogram is automatically set, we can always take control and choose a higher (or lower) number of bins for more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the residuals with more bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the histograms above that the distribution of residuals follows a more or less normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Optional**\n",
    "\n",
    "Another wide-spread tool to verify that residuals are normally distributed is to use a normal Q-Q plot. The Qs in Q-Q plot stand for quantile. A **quantile** divides a frequency distribution into equal groups, each containing the same fraction of the total population. For example we could divide our data into two groups, each containing exactly 50% of the data. This would be a quantile, and it's known as the median. We could also split our data into groups containing exactly 25% of the data each, something known as quartiles. \n",
    "\n",
    "The idea of a normal Q-Q plot is to compare the quantiles of our distribution (in our case the residuals) with those of a pure normal distribution. If the quantiles match (meaning they form a perfect diagonal line), then our distribution resembles a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "qq = qqplot(residuals, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the points line up nicely along a straight diagonal line, confirming that our distribution resembles a normal distribution. If you choose to use Q-Q plots in your own projects and find that the points don't lie in a straight line you can use [this](https://seankross.com/2016/02/29/A-Q-Q-Plot-Dissection-Kit.html) nice article to understand why. The code is written in `R` instead of `Python` but the graphs should be self-explanatory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoscedasticity and Independence\n",
    "\n",
    "This two assumptions can be verified with the same plot, namely a scatterplot of the residual against the prediction. Let's review what each of these mean and what we're looking for:\n",
    "\n",
    "* **Homoscedasticity**, or in other words constant variance of the residuals. It is important to verify, or at least understand, how our residuals' variance varies with the magnitude of the predictions. There are cases where the predictions are more accurate (less variance) on one extreme of the range of predictions and less accurate (more variance) on the other extreme.\n",
    "\n",
    "* **Independence**. This assumption is most relevant when dealing with time series data. What we are looking here is for 'randomness' in the residuals, meaning no autocorrelation. Autocorrelation may reveal itself in this plot as any noticeable pattern that would allow us to predict the next residual.\n",
    "\n",
    "We already have all the variables we need: the predictions for the entire dataset and the associated residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a scatterplot of residuals vs. predicted value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no noticeable change in the variance of the residuals based on the magnitude of the predictions, so we have homoscedasticity.\n",
    "\n",
    "In addition there is no apparent pattern to the residuals that would imply autocorrelation and so we have independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1><center>Break</center></h1>\n",
    "<center>That concludes the first step of our linear regression pipeline. We have loaded and cleaned the data, visually inspected the variables to choose the best candidate for simple linear regression, trained a model and measured the accuracy of its predictions, and verified all assumptions.</center><br>\n",
    "\n",
    "<center>Now it's time to see how we can make use of more variables to enhance the predictive power of the model by moving from simple linear regression to <b>multiple linear regression</b>.</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression and Multicollinearity\n",
    "\n",
    "We are now ready to incorporate more variables to our regression model, but we have to be **really careful** about **multicollinearity**. Thus the first thing we do is to make sure that the variables we plan to include are not mutually correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitatively assessing multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways we can check. The first is to look at all the scatterplots between independent variables. Happily we already know how to do that as it is part of the output of `seaborn`'s `pairplot`. Let's bring it back once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring back the pairplot (once again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be any correlation between the independent variables, which is a really good sign. In this case it seems very clear-cut but in other cases and other datasets it may not be so obvious from the scatterplots. So how can we quantitatively check for multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitatively assessing multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can go about this is to look at the **correlation matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the correlation matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information in the table above is more specific, but a bit harder to read. We can use the `heatmap` function from the `seaborn` library to get the visual and numeric benefits of both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an annotated heatmap of the correlations. The \"annot\" parameter tells the plot to display labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lighter colours indicate correlations closer to 1 (strong positive correlation), while darker colours indicate correlations closer to -1 (strong negative correlation). We call the specific value for each correlation the \"correlation coefficient\". It gives us both the magnitude and direction of a relationship between two variables.\n",
    "\n",
    "A good rule of thumb of what constitutes multicollinearity is if the value of the coefficient is greater than +0.6 or smaller than -0.6. In this case we are happily free of multicollinearity and we can continue with our analysis.\n",
    "\n",
    "If you encounter multicollinearity in your own projects there are several alternatives available. Some of the most common ones are:\n",
    "\n",
    "* Removing one of the highly correlated variables\n",
    "* Combining both correlated variables into one\n",
    "\n",
    "You can read more on multicollinearity [here](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to implement our multiple linear regression. Choose which variables you'd like to include in the model and follow the same steps from the simple linear regression section above to define your `X` and `y` variables, split them into a train and a test set, define and fit a multiple linear regression (the syntax is exactly the same as in the case of simple linear regression, the only thing that changes is what we pass to the `.fit()` method!), generate predictions on the test set and evaluate the error metric or metrics of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `X` and `y` variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and a testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit a linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and interpret the intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print and interpret the coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions for the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
